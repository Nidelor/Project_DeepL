{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFERT LEARNING USING THE SOUNDNET MODEL ON THE GTZAN DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile, torch\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Here we are defining the model and loading the pre-trained soundnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_model import SoundNet8_pytorch\n",
    "from utils import vector_to_scenes,vector_to_obj\n",
    "\n",
    "## define the soundnet model\n",
    "model = SoundNet8_pytorch()\n",
    "\n",
    "## Load the weights of the pretrained model\n",
    "model.load_state_dict(torch.load('sound8.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define the all the data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = {\"blues\":0,\"classical\":1,\"country\":2,\"disco\":3,\"hiphop\":4,\"jazz\":5,\"metal\":6,\"pop\":7,\"reggae\":8,\"rock\":9}\n",
    "\n",
    "# Get all the audio file path\n",
    "df = pd.read_csv(\"features_30_sec.csv\")\n",
    "list_filename = list(df.filename)\n",
    "\n",
    "# Split the list in sublists by genre\n",
    "list_filepath = []\n",
    "for i in range(10):\n",
    "    list_filepath.append(list_filename[100*i: 100*(i+1)])\n",
    "\n",
    "# Complete all the paths\n",
    "list_audiofile_path = []\n",
    "\n",
    "for genre, key in genre_dict.items():\n",
    "    sublist = []\n",
    "    \n",
    "    for filepath in list_filepath[key]:\n",
    "        if filepath != \"\\genre_original\\blues\\blues_00054.wav\":\n",
    "            path = os.path.join(r\".\\genres_original\", genre , filepath )\n",
    "            sublist.append(path)\n",
    "        \n",
    "    list_audiofile_path.append(sublist)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to isolate the fifth layer features and then re-classify them with a classifier method\n",
    "\n",
    "After testing we find that the dimension of the fifth layer features are (1,256,81,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the features wanted from the soundnet modelling of the given audiofile_path\n",
    "\n",
    "def get_soundnet_features(audiofile_path, n_feature):\n",
    "    waveform, sr = torchaudio.load(audiofile_path)\n",
    "    # Reshape the data to the format expected by the model (Batch, 1, time, 1)\n",
    "    waveform = waveform.view(1,1,-1,1)\n",
    "\n",
    "    # Extract the features we want\n",
    "    features = model.extract_feat(waveform)  #features 7 and 8 are object_pred and scene_pred\n",
    "\n",
    "    # Get the good features\n",
    "    feature = features[n_feature]\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the fifth layer features and so, create the dataset we will perform the classification on\n",
    "\n",
    "def create_dataset(list_of_path, genre_dict, n_feature):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for genre, key in genre_dict.items():\n",
    "        for path in list_of_path[key]:\n",
    "            if path != '.\\\\genres_original\\\\jazz\\\\jazz.00054.wav':\n",
    "                X.append(np.array(get_soundnet_features(path, n_feature-1)))\n",
    "                Y.append(genre)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_5,y_5 = create_dataset(list_audiofile_path, genre_dict, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:576: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (256,81,1) into shape (1,256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\quent\\Desktop\\FISEA2-MCE\\UE_D_Deep_Learning\\GIT\\Project_DeepL\\gtzan_init.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/quent/Desktop/FISEA2-MCE/UE_D_Deep_Learning/GIT/Project_DeepL/gtzan_init.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_x_5 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(x_5)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/quent/Desktop/FISEA2-MCE/UE_D_Deep_Learning/GIT/Project_DeepL/gtzan_init.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_x_5\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mx_5.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/quent/Desktop/FISEA2-MCE/UE_D_Deep_Learning/GIT/Project_DeepL/gtzan_init.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_y_5 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(y_5)\n",
      "File \u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:737\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    729\u001b[0m         mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    730\u001b[0m             arrays,\n\u001b[0;32m    731\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    734\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    735\u001b[0m         )\n\u001b[0;32m    736\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 737\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    738\u001b[0m             data,\n\u001b[0;32m    739\u001b[0m             index,\n\u001b[0;32m    740\u001b[0m             columns,\n\u001b[0;32m    741\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    742\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    743\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    744\u001b[0m         )\n\u001b[0;32m    745\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    746\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    747\u001b[0m         {},\n\u001b[0;32m    748\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    751\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    752\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:331\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    326\u001b[0m         values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    328\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m     \u001b[39m# by definition an array here\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     \u001b[39m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m     values \u001b[39m=\u001b[39m _prep_ndarray(values, copy\u001b[39m=\u001b[39;49mcopy_on_sanitize)\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_equal(values\u001b[39m.\u001b[39mdtype, dtype):\n\u001b[0;32m    334\u001b[0m     shape \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:576\u001b[0m, in \u001b[0;36m_prep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[39m# we could have a 1-dim or 2-dim list here\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[39m# this is equiv of np.asarray, but does object conversion\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[39m# and platform dtype preservation\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(values[\u001b[39m0\u001b[39m]):\n\u001b[1;32m--> 576\u001b[0m     values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray([convert(v) \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m values])\n\u001b[0;32m    577\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(values[\u001b[39m0\u001b[39m], np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m values[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    578\u001b[0m     \u001b[39m# GH#21861 see test_constructor_list_of_lists\u001b[39;00m\n\u001b[0;32m    579\u001b[0m     values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([convert(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m values])\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (256,81,1) into shape (1,256)"
     ]
    }
   ],
   "source": [
    "#df_x_5 = pd.DataFrame(x_5)\n",
    "#df_x_5.to_csv(\"x_5.csv\")\n",
    "\n",
    "#df_y_5 = pd.DataFrame(y_5)\n",
    "#df_y_5.to_csv(\"y_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_5\n",
    "y = y_5\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for idx, xi in enumerate(x):\n",
    "    if xi.shape == x[0].shape:\n",
    "        X.append(xi)\n",
    "        Y.append(y[idx])\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "Y = np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traintestsplit le dataset fait\n",
    "# chosir quelle couche je prend de soundet pour le transfert learning fait\n",
    "# train un nouveau modèle simple (uen simple classification sur scikit-learn)\n",
    "# comparer les résultats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION \n",
    "### We will try different types of classifiers in order to get the best results for the classification and then compare it with the results of these classifiers without this transfert learning process\n",
    "\n",
    "#### UPDATE : AFTER TESTING SOME CLASSIFIER, WE CHOOSE TO USE THE SVM AS PRECONISED IN THE WORKPAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "svm = SVC(decision_function_shape=\"ovo\")\n",
    "\n",
    "def classification(model, x_train, x_test, y_train, y_test):\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  7  2  4  1  0  3  1 10  9]\n",
      " [ 1 29  1  0  0  1  1  1  1 10]\n",
      " [ 6 11  6  3  0  0  4  0  5 13]\n",
      " [10  5  2  8  1  0  5  5  9  8]\n",
      " [11  2  1  4  2  1  4  2 12  4]\n",
      " [ 5 25  4  0  0  9  2  1  5  5]\n",
      " [ 5 16  5  0  1  0  8  2  4 11]\n",
      " [ 1  2  2  0  2  1  3 19  4 18]\n",
      " [ 8  1  2  5  0  0  2  2 20  5]\n",
      " [ 5  8  4  2  0  1  6  2  4 12]]\n",
      "0.2581967213114754\n"
     ]
    }
   ],
   "source": [
    "# SVM (poly)\n",
    "model = SVC(C = 2, kernel = \"poly\")\n",
    "classification(model, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOTHER TRY WITH THE FEATURES FROM THE ANTEPENULTIMATE LAYER (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975\n",
      "(975, 21504)\n",
      "(975,)\n",
      "[[ 4  1  9  3  3  0 10  0  6  1]\n",
      " [ 1  4 17  0  1  0 11  0  0  0]\n",
      " [ 1  0 16  2  2  0  8  0  2  1]\n",
      " [ 0  0  6  5  7  0  9  0  7  1]\n",
      " [ 1  1  2  1 13  0  7  0  3  0]\n",
      " [ 2  6 13  3  1  2  8  0  3  0]\n",
      " [ 0  0  9  1  3  0 14  0  1  1]\n",
      " [ 2  2 13  8  2  0 11  2  1  1]\n",
      " [ 4  1  7  7  6  0  4  0  5  3]\n",
      " [ 1  2 11  4  2  0  7  0  2  1]]\n",
      "0.19298245614035087\n"
     ]
    }
   ],
   "source": [
    "## define the soundnet model\n",
    "model = SoundNet8_pytorch()\n",
    "\n",
    "## Load the weights of the pretrained model\n",
    "model.load_state_dict(torch.load('sound8.pth'))\n",
    "\n",
    "x_7,y_7 = create_dataset(list_audiofile_path, genre_dict, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975\n",
      "(975, 21504)\n",
      "(975,)\n",
      "[[14  6  0  8  0  1  0  2  9  7]\n",
      " [11 14  0  1  0  1  1  5  0 11]\n",
      " [ 9  6  2  3  0  1  1  8  4 17]\n",
      " [ 7  3  0 12  0  0  1 10  5 10]\n",
      " [ 8  3  0 20  1  1  2  2  8  8]\n",
      " [12  8  0  7  0  6  0  3  4  7]\n",
      " [11  4  1  2  0  0  2  3  4 25]\n",
      " [ 7  6  1  9  0  0  0 16  3  7]\n",
      " [10  2  0 19  0  0  1  1 11  6]\n",
      " [ 6 11  0  4  0  0  3  8  5 10]]\n",
      "0.18032786885245902\n"
     ]
    }
   ],
   "source": [
    "x = x_7\n",
    "y = y_7\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for idx, xi in enumerate(x):\n",
    "    if xi.shape == x[0].shape:\n",
    "        X.append(xi)\n",
    "        Y.append(y[idx])\n",
    "\n",
    "\n",
    "print(len(X))\n",
    "\n",
    "X = np.array(X)\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "print(X.shape)\n",
    "Y = np.array(Y)\n",
    "print(Y.shape)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5)\n",
    "\n",
    "# SVM (poly)\n",
    "model = SVC(C = 2, kernel = \"poly\")\n",
    "classification(model, x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a771b7766c439f167a85454aa45440eda3d18649461135719fffb2235b2f5b6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
