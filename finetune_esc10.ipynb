{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile, torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_model import SoundNet8_pytorch\n",
    "from utils import vector_to_scenes,vector_to_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.weight', '0.bias', '1.weight', '1.bias', '1.running_mean', '1.running_var', '1.num_batches_tracked'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = SoundNet8_pytorch()\n",
    "#'conv5.0.weight', 'conv5.0.bias', 'conv5.1.weight', 'conv5.1.bias', 'conv5.1.running_mean', 'conv5.1.running_var', 'conv5.1.num_batches_tracked'\n",
    "state_dict = torch.load('sound8.pth')\n",
    "\n",
    "state_dict_5 = {}\n",
    "\n",
    "state_dict_5['0.weight'] = state_dict['conv5.0.weight']\n",
    "state_dict_5['0.bias'] = state_dict['conv5.0.bias']\n",
    "state_dict_5['1.weight'] = state_dict['conv5.1.weight']\n",
    "state_dict_5['1.bias'] = state_dict['conv5.1.bias']\n",
    "state_dict_5['1.running_mean'] = state_dict['conv5.1.running_mean']\n",
    "state_dict_5['1.running_var'] = state_dict['conv5.1.running_var']\n",
    "state_dict_5['1.num_batches_tracked'] = state_dict['conv5.1.num_batches_tracked']\n",
    "\n",
    "test.conv5.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fineTune_SoundNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fineTune_SoundNet, self).__init__()\n",
    "\n",
    "        self.model = SoundNet8_pytorch()\n",
    "        self.model.conv5.load_state_dict(state_dict_5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for net in [self.model.conv1, self.model.conv2, self.model.conv3, self.model.conv4]:\n",
    "            x = net(x)\n",
    "        \n",
    "        x = self.model.conv5(x)\n",
    "        \n",
    "        for net in [self.model.conv6, self.model.conv7]:\n",
    "            x = net(x)\n",
    "\n",
    "        object_pred = self.model.conv8(x)\n",
    "        scene_pred = self.model.conv8_2(x)\n",
    "        return object_pred, scene_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fineTune_SoundNet()\n",
    "\n",
    "#Freeze the parameters of the 5th layer\n",
    "for param in model.model.conv5.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [20, 16, 55126, 1]           1,040\n",
      "       BatchNorm2d-2         [20, 16, 55126, 1]              32\n",
      "              ReLU-3         [20, 16, 55126, 1]               0\n",
      "         MaxPool2d-4          [20, 16, 6890, 1]               0\n",
      "            Conv2d-5          [20, 32, 3446, 1]          16,416\n",
      "       BatchNorm2d-6          [20, 32, 3446, 1]              64\n",
      "              ReLU-7          [20, 32, 3446, 1]               0\n",
      "         MaxPool2d-8           [20, 32, 430, 1]               0\n",
      "            Conv2d-9           [20, 64, 216, 1]          32,832\n",
      "      BatchNorm2d-10           [20, 64, 216, 1]             128\n",
      "             ReLU-11           [20, 64, 216, 1]               0\n",
      "           Conv2d-12          [20, 128, 109, 1]          65,664\n",
      "      BatchNorm2d-13          [20, 128, 109, 1]             256\n",
      "             ReLU-14          [20, 128, 109, 1]               0\n",
      "           Conv2d-15           [20, 256, 55, 1]         131,328\n",
      "      BatchNorm2d-16           [20, 256, 55, 1]             512\n",
      "             ReLU-17           [20, 256, 55, 1]               0\n",
      "        MaxPool2d-18           [20, 256, 13, 1]               0\n",
      "           Conv2d-19            [20, 512, 7, 1]         524,800\n",
      "      BatchNorm2d-20            [20, 512, 7, 1]           1,024\n",
      "             ReLU-21            [20, 512, 7, 1]               0\n",
      "           Conv2d-22           [20, 1024, 4, 1]       2,098,176\n",
      "      BatchNorm2d-23           [20, 1024, 4, 1]           2,048\n",
      "             ReLU-24           [20, 1024, 4, 1]               0\n",
      "           Conv2d-25           [20, 1000, 2, 1]       1,025,000\n",
      "           Conv2d-26            [20, 401, 2, 1]         411,025\n",
      "================================================================\n",
      "Total params: 4,310,345\n",
      "Trainable params: 4,178,505\n",
      "Non-trainable params: 131,840\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8.41\n",
      "Forward/backward pass size (MB): 496.77\n",
      "Params size (MB): 16.44\n",
      "Estimated Total Size (MB): 521.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(1,110250, 1), batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import vector_to_scenes,vector_to_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-110389-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>110389</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-116765-A-41.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>chainsaw</td>\n",
       "      <td>True</td>\n",
       "      <td>116765</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-17150-A-12.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>crackling_fire</td>\n",
       "      <td>True</td>\n",
       "      <td>17150</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-172649-A-40.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>helicopter</td>\n",
       "      <td>True</td>\n",
       "      <td>172649</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>5-233160-A-1.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>rooster</td>\n",
       "      <td>True</td>\n",
       "      <td>233160</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>5-234879-A-1.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>rooster</td>\n",
       "      <td>True</td>\n",
       "      <td>234879</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>5-234879-B-1.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>rooster</td>\n",
       "      <td>True</td>\n",
       "      <td>234879</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>5-235671-A-38.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>clock_tick</td>\n",
       "      <td>True</td>\n",
       "      <td>235671</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>5-9032-A-0.wav</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>9032</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename  fold  target        category  esc10  src_file take\n",
       "index                                                                       \n",
       "0       1-100032-A-0.wav     1       0             dog   True    100032    A\n",
       "1       1-110389-A-0.wav     1       0             dog   True    110389    A\n",
       "2      1-116765-A-41.wav     1      41        chainsaw   True    116765    A\n",
       "3       1-17150-A-12.wav     1      12  crackling_fire   True     17150    A\n",
       "4      1-172649-A-40.wav     1      40      helicopter   True    172649    A\n",
       "...                  ...   ...     ...             ...    ...       ...  ...\n",
       "395     5-233160-A-1.wav     5       1         rooster   True    233160    A\n",
       "396     5-234879-A-1.wav     5       1         rooster   True    234879    A\n",
       "397     5-234879-B-1.wav     5       1         rooster   True    234879    B\n",
       "398    5-235671-A-38.wav     5      38      clock_tick   True    235671    A\n",
       "399       5-9032-A-0.wav     5       0             dog   True      9032    A\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('meta\\esc10.csv')\n",
    "df = df.rename(columns={'Unnamed: 0': 'index'})\n",
    "df = df.set_index('index')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESC10_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file, audio_dir):\n",
    "        self.annotations = pd.read_csv(annotations_file).rename(columns={'Unnamed: 0': 'index'}).set_index('index')\n",
    "        self.audio_dir = audio_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        if sr != 22050:\n",
    "            transform = Resample(sr,22050)\n",
    "            signal = transform(signal)\n",
    "        if signal.shape[0]>1:\n",
    "            signal = torch.mean(signal,axis=0)\n",
    "        signal = signal.view(1,-1,1)\n",
    "        return signal,label\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        return os.path.join(self.audio_dir, self.annotations.iloc[index, 0])\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 3]\n",
    "\n",
    "esc10 = ESC10_Dataset(\"meta\\esc10.csv\", \"audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # how many samples per batch to load\n",
    "valid_size = 0.2 # percentage of training set to use as validation\n",
    "\n",
    "num_train = len(esc10)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_index, valid_index = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(valid_index)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(esc10, batch_size = batch_size, sampler = train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(esc10, batch_size = batch_size, sampler = valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.KLDivLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remap the label into int so that pytorch stop bugging because it hates string apparently...\n",
    "label_mapping_dataset = {'dog':0, 'chainsaw':1, 'crackling_fire':2, 'helicopter':3, 'rain':4, 'crying_baby':5, 'clock_tick':6, 'sneezing':7, 'rooster':8, 'sea_waves':9}\n",
    "\n",
    "label_mapping_model = {'n02085620 Chihuahua':0, 'n03000684 chain saw, chainsaw':1, 'n03729826 matchstick':2, 'n03345487 fire engine, fire truck':3, 'n04049303 rain barrel':4, 'n03825788 nipple':5, 'n02708093 analog clock':6, 'n03424325 gasmask, respirator, gas helmet':7, 'n01514668 cock':8, 'n04557648 water bottle':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "def training(n_epochs, train_loader, valid_loader, model, criterion, optimizer):\n",
    "\n",
    "    train_losses, valid_losses = [], []\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, valid_loss = 0, 0 # monitor losses\n",
    "      \n",
    "        # train the model\n",
    "        model.train() # prep model for training\n",
    "        for data, label in train_loader:\n",
    "            label = torch.tensor([label_mapping_dataset[x] for x in label], dtype=float)\n",
    "            optimizer.zero_grad() # clear the gradients of all optimized variables\n",
    "            output = model(data) # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            pred = vector_to_obj(output[1].detach().numpy())\n",
    "            if pred in label_mapping_model.keys():\n",
    "                pred = label_mapping_model[pred] + 0.0\n",
    "            else:\n",
    "                pred = 11.\n",
    "            loss = criterion(torch.tensor(pred, dtype=float), label) # calculate the loss\n",
    "            loss.backward() # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            optimizer.step() # perform a single optimization step (parameter update)\n",
    "            train_loss += loss.item() * data.size(0) # update running training loss\n",
    "      \n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        for data, label in valid_loader:\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            pred = output[1]\n",
    "            target = torch.tensor(label_mapping_dataset[label[0]]).to(torch.float)\n",
    "            loss = criterion(torch.tensor(pred), target) # calculate the loss\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "      \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss /= len(train_loader.sampler)\n",
    "        valid_loss /= len(valid_loader.sampler)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "      \n",
    "        print('epoch: {} \\ttraining Loss: {:.6f} \\tvalidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    return train_losses, valid_losses      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tev\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "kl_div: Integral inputs not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tev\\OneDrive\\Bureau\\IMTA\\MCE\\DL\\Projet_DeepL\\finetune_esc10.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tev/OneDrive/Bureau/IMTA/MCE/DL/Projet_DeepL/finetune_esc10.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_losses_1, valid_losses_1 \u001b[39m=\u001b[39m training(epochs, train_loader, valid_loader, model, criterion, optimizer)\n",
      "\u001b[1;32mc:\\Users\\Tev\\OneDrive\\Bureau\\IMTA\\MCE\\DL\\Projet_DeepL\\finetune_esc10.ipynb Cell 19\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(n_epochs, train_loader, valid_loader, model, criterion, optimizer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tev/OneDrive/Bureau/IMTA/MCE/DL/Projet_DeepL/finetune_esc10.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tev/OneDrive/Bureau/IMTA/MCE/DL/Projet_DeepL/finetune_esc10.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39m11.\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Tev/OneDrive/Bureau/IMTA/MCE/DL/Projet_DeepL/finetune_esc10.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(torch\u001b[39m.\u001b[39;49mtensor(pred), label) \u001b[39m# calculate the loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tev/OneDrive/Bureau/IMTA/MCE/DL/Projet_DeepL/finetune_esc10.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward() \u001b[39m# backward pass: compute gradient of the loss with respect to model parameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tev/OneDrive/Bureau/IMTA/MCE/DL/Projet_DeepL/finetune_esc10.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m# perform a single optimization step (parameter update)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tev\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Tev\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:471\u001b[0m, in \u001b[0;36mKLDivLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mkl_div(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction, log_target\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_target)\n",
      "File \u001b[1;32mc:\\Users\\Tev\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2928\u001b[0m, in \u001b[0;36mkl_div\u001b[1;34m(input, target, size_average, reduce, reduction, log_target)\u001b[0m\n\u001b[0;32m   2925\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2926\u001b[0m         reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m-> 2928\u001b[0m reduced \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mkl_div(\u001b[39minput\u001b[39;49m, target, reduction_enum, log_target\u001b[39m=\u001b[39;49mlog_target)\n\u001b[0;32m   2930\u001b[0m \u001b[39mif\u001b[39;00m reduction \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatchmean\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2931\u001b[0m     reduced \u001b[39m=\u001b[39m reduced \u001b[39m/\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: kl_div: Integral inputs not supported."
     ]
    }
   ],
   "source": [
    "train_losses_1, valid_losses_1 = training(epochs, train_loader, valid_loader, model, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8759a9ff7705a9ce9cb660f47fe0fac0d5301f1ccc205dcb3f658ee04f2809e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
